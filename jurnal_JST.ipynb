{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nurra0210/Tugas-Besar-JST/blob/main/jurnal_JST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iCvEKlVwofzk"
      },
      "source": [
        "IMPORT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KulfoJn6RFOM"
      },
      "outputs": [],
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import minmax_scale\n",
        "from sklearn.metrics import accuracy_score\n",
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cAGr6nVKoilD"
      },
      "source": [
        "PREPROCESSING DATASET"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UUJk05pKRNE6"
      },
      "outputs": [],
      "source": [
        "# read data\n",
        "dataset = pd.read_csv('/content/dataset-vertebral column.csv')\n",
        "\n",
        "# set data & label\n",
        "X = dataset.iloc[:, :-1].values # menampung data\n",
        "Y = dataset.iloc[:, -1].values # menampung label\n",
        "\n",
        "# MinMax Normalization\n",
        "X = minmax_scale(X)\n",
        "\n",
        "# One-hot Encoding\n",
        "Y_encode = []\n",
        "for index,i in enumerate(Y.tolist()):\n",
        "  if i == 1: Y_encode.append([1,0,0])\n",
        "  if i == 2: Y_encode.append([0,1,0])\n",
        "  if i == 3: Y_encode.append([0,0,1])\n",
        "\n",
        "# Split training & testing data\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, Y_encode, test_size=.3)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LcRG4Unq64H_",
        "outputId": "6af86173-0c6c-4681-c825-1f3ed014545e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Bcswu_3ooJv"
      },
      "source": [
        "TRAINING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_FwnAPB2RPzr"
      },
      "outputs": [],
      "source": [
        "# Sigmoid activation\n",
        "def sigmoid(x):\n",
        "  return 1/(1 + np.exp(-x))\n",
        "sig = np.vectorize(sigmoid)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wi2S6IdVRisF"
      },
      "outputs": [],
      "source": [
        "# Nguyen-Widraw\n",
        "def nguyen_widrow(n,p):\n",
        "  # n = 6 #input\n",
        "  # p = 5 #hidden\n",
        "\n",
        "  weight = np.random.uniform(low=-5, high=5, size=(p,n))\n",
        "  beta = .7*(p**(1/n))\n",
        "  bias = np.random.uniform(low=-beta, high=beta, size=p)\n",
        "  norm_hidden = []\n",
        "\n",
        "  for i in range(p):\n",
        "    jum = 0\n",
        "    for j in range(n):\n",
        "      jum += weight[i][j]**2\n",
        "    norm_hidden.append(jum**(1/2))\n",
        "  \n",
        "  for i in range(p):\n",
        "    for j in range(n):\n",
        "      weight[i][j] = beta * weight[i][j] / norm_hidden[i]\n",
        "\n",
        "  return weight, bias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cZ48UaG7RTr5"
      },
      "outputs": [],
      "source": [
        "def model_fit(hidden_layer, input_layer, output_layer, x_train, y_train, epoch, nw):\n",
        "  loss_values = []\n",
        "  acc_values = []\n",
        "\n",
        "  # inisialisasi bobot dan bias: hidden layer\n",
        "  if nw:\n",
        "    w_hidden, b_hidden = nguyen_widrow(input_layer,hidden_layer)\n",
        "  else:\n",
        "    # w_hidden = np.random.uniform(low=-5, high=5, size=(hidden_layer,input_layer))\n",
        "    # b_hidden = np.random.uniform(low=-5, high=5, size=hidden_layer)\n",
        "    w_hidden = np.random.uniform(low=-1, high=1, size=(hidden_layer,input_layer))\n",
        "    b_hidden = np.random.uniform(low=-1, high=1, size=hidden_layer)\n",
        "\n",
        "  # inisialisasi bobot dan bias: Output layer\n",
        "  w_output = np.random.uniform(low=-5, high=5, size=(output_layer,hidden_layer))\n",
        "  b_output = np.random.uniform(low=-5, high=5, size=output_layer)\n",
        "  # w_output, b_output = nguyen_widrow(hidden_layer,output_layer)\n",
        "\n",
        "  for i in range(epoch):\n",
        "    mse = 0\n",
        "    acc = 0\n",
        "    for index,data in enumerate(x_train):\n",
        "      # === FEEDFOWARD ===\n",
        "      # ---- feedfoward: hidden layer\n",
        "      o_hidden = np.matmul(w_hidden, data) + b_hidden\n",
        "      o_hidden = sig(o_hidden)\n",
        "      # ---- feedfoward: output layer\n",
        "      o_output = np.matmul(w_output, o_hidden) + b_output\n",
        "      o_output = sig(o_output)\n",
        "\n",
        "      # === BACKPROPAGATION ===\n",
        "      # ---- error: output layer\n",
        "      e_output = (y_train[index]-o_output) * o_output * (1 - o_output)\n",
        "      # ---- delta bobot: output layer\n",
        "      deltaW_output = lr * (e_output * o_hidden[np.newaxis].T)\n",
        "      # ---- delta bias: output layer\n",
        "      deltaB_output = lr * e_output\n",
        "\n",
        "      # ---- error: hidden layer\n",
        "      e_hidden = (np.matmul(np.array(w_output).T.tolist(), e_output)) * o_hidden * (1 - o_hidden)\n",
        "      # ---- delta bobot: hidden layer\n",
        "      deltaW_hidden = lr * (e_hidden * np.array(data)[np.newaxis].T)\n",
        "      # ---- delta bias: hidden layer\n",
        "      deltaB_hidden = lr * e_hidden\n",
        "\n",
        "      # ---- update bobot: output layer\n",
        "      w_output = w_output + deltaW_output.transpose()\n",
        "      # ---- update bias: output layer\n",
        "      b_output = b_output + deltaB_output\n",
        "\n",
        "      # ---- update bobot: hidden layer\n",
        "      w_hidden = w_hidden + deltaW_hidden.transpose()\n",
        "      # ---- update bias: hidden layer\n",
        "      b_hidden = b_hidden + deltaB_hidden\n",
        "\n",
        "      mse += sum((y_train[index]-o_output) ** 2)\n",
        "      acc += sum(np.absolute(y_train[index]-o_output.round()))\n",
        "    \n",
        "    mse /= len(x_train)\n",
        "    acc = 1 - (acc/len(y_train))\n",
        "    if mse < 0.01: break\n",
        "    else:\n",
        "      loss_values.append(mse)\n",
        "      acc_values.append(acc)\n",
        "      # print(\"Epoch:\",i,\" | loss :\",mse,\" | accuracy :\",acc)\n",
        "      print(\"Epoch:\",i,\" | loss :\",mse)\n",
        "  \n",
        "  return w_hidden, b_hidden, w_output, b_output, loss_values, acc_values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "etRFMxX2oq8T"
      },
      "source": [
        "TESTING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7GOfMrp-RU36"
      },
      "outputs": [],
      "source": [
        "def predict(x_test, w_hidden, b_hidden, w_output, b_output):\n",
        "  acc = 0\n",
        "  o_predict = []\n",
        "  o_predict2 = []\n",
        "  for index,data in enumerate(x_test):\n",
        "    # === FEEDFOWARD ===\n",
        "    # ---- feedfoward: hidden layer\n",
        "    o_hidden = sig(np.matmul(w_hidden, data) + b_hidden)\n",
        "    # ---- feedfoward: output layer\n",
        "    o_output = sig(np.matmul(w_output, o_hidden) + b_output)\n",
        "    o_predict.append((np.round(o_output)).tolist())\n",
        "    o_predict2.append(o_output)\n",
        "\n",
        "    if y_test[index]==(np.round(o_output)).tolist(): acc+=1\n",
        "  # print('predict : ',acc/len(x_test))\n",
        "\n",
        "  return acc/len(x_test), o_predict, o_predict2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XAU8NukjotTb"
      },
      "source": [
        "TUNING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AMSnnIHQRXgS",
        "outputId": "a6d67b84-b1b5-41b3-acfa-9d1723340a02"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0  | loss : 0.6679925142178159\n",
            "Epoch: 1  | loss : 0.5294821999940821\n",
            "Epoch: 2  | loss : 0.4675268226472447\n",
            "Epoch: 3  | loss : 0.43813256665769695\n",
            "Epoch: 4  | loss : 0.4201688735575157\n",
            "Epoch: 5  | loss : 0.4063145017285187\n",
            "Epoch: 6  | loss : 0.3947152860471586\n",
            "Epoch: 7  | loss : 0.38473247608058325\n",
            "Epoch: 8  | loss : 0.37605712518613577\n",
            "Epoch: 9  | loss : 0.36848256279685504\n",
            "Epoch: 10  | loss : 0.3618368588102013\n",
            "Epoch: 11  | loss : 0.35596946421498116\n",
            "Epoch: 12  | loss : 0.3507515732391666\n",
            "Epoch: 13  | loss : 0.3460757308090153\n",
            "Epoch: 14  | loss : 0.34185333353674896\n",
            "Epoch: 15  | loss : 0.33801158212772486\n",
            "Epoch: 16  | loss : 0.334490716864564\n",
            "Epoch: 17  | loss : 0.3312416655653797\n",
            "Epoch: 18  | loss : 0.3282240475297337\n",
            "Epoch: 19  | loss : 0.32540448776753\n",
            "Epoch: 20  | loss : 0.3227552142222326\n",
            "Epoch: 21  | loss : 0.3202529057737234\n",
            "Epoch: 22  | loss : 0.31787774734125884\n",
            "Epoch: 23  | loss : 0.31561264409862766\n",
            "Epoch: 24  | loss : 0.3134425525084008\n",
            "Epoch: 25  | loss : 0.311353898556411\n",
            "Epoch: 26  | loss : 0.30933406907476985\n",
            "Epoch: 27  | loss : 0.30737097712325506\n",
            "Epoch: 28  | loss : 0.3054527149218345\n",
            "Epoch: 29  | loss : 0.3035673165247137\n",
            "Epoch: 30  | loss : 0.30170265676891506\n",
            "Epoch: 31  | loss : 0.2998465131497151\n",
            "Epoch: 32  | loss : 0.2979868134445988\n",
            "Epoch: 33  | loss : 0.29611208317299825\n",
            "Epoch: 34  | loss : 0.2942120894502365\n",
            "Epoch: 35  | loss : 0.29227864478193455\n",
            "Epoch: 36  | loss : 0.29030648143159354\n",
            "Epoch: 37  | loss : 0.2882940419503389\n",
            "Epoch: 38  | loss : 0.2862439814321162\n",
            "Epoch: 39  | loss : 0.2841631818395391\n",
            "Epoch: 40  | loss : 0.28206216781761556\n",
            "Epoch: 41  | loss : 0.27995397841321923\n",
            "Epoch: 42  | loss : 0.2778527293213889\n",
            "Epoch: 43  | loss : 0.2757722012718692\n",
            "Epoch: 44  | loss : 0.27372474634778937\n",
            "Epoch: 45  | loss : 0.2717206429919588\n",
            "Epoch: 46  | loss : 0.26976785759211264\n",
            "Epoch: 47  | loss : 0.267872074188762\n",
            "Epoch: 48  | loss : 0.266036843594591\n",
            "Epoch: 49  | loss : 0.26426374202948233\n",
            "Epoch: 50  | loss : 0.26255248837626044\n",
            "Epoch: 51  | loss : 0.26090103358519123\n",
            "Epoch: 52  | loss : 0.2593056847610072\n",
            "Epoch: 53  | loss : 0.2577613358041407\n",
            "Epoch: 54  | loss : 0.2562618423760139\n",
            "Epoch: 55  | loss : 0.2548005271041732\n",
            "Epoch: 56  | loss : 0.2533707649770937\n",
            "Epoch: 57  | loss : 0.25196659076250083\n",
            "Epoch: 58  | loss : 0.2505832738603265\n",
            "Epoch: 59  | loss : 0.24921779946836783\n",
            "Epoch: 60  | loss : 0.24786917779393777\n",
            "Epoch: 61  | loss : 0.2465384994344527\n",
            "Epoch: 62  | loss : 0.24522869065468358\n",
            "Epoch: 63  | loss : 0.2439439973497349\n",
            "Epoch: 64  | loss : 0.24268930881668177\n",
            "Epoch: 65  | loss : 0.24146947737090269\n",
            "Epoch: 66  | loss : 0.24028877295853662\n",
            "Epoch: 67  | loss : 0.23915054705298355\n",
            "Epoch: 68  | loss : 0.23805710578300543\n",
            "Epoch: 69  | loss : 0.23700974180600207\n",
            "Epoch: 70  | loss : 0.2360088583374427\n",
            "Epoch: 71  | loss : 0.23505412805259762\n",
            "Epoch: 72  | loss : 0.2341446493699729\n",
            "Epoch: 73  | loss : 0.23327908173813305\n",
            "Epoch: 74  | loss : 0.23245575504610888\n",
            "Epoch: 75  | loss : 0.23167275569764237\n",
            "Epoch: 76  | loss : 0.23092799473581277\n",
            "Epoch: 77  | loss : 0.23021926350310792\n",
            "Epoch: 78  | loss : 0.22954428112299044\n",
            "Epoch: 79  | loss : 0.22890073653282722\n",
            "Epoch: 80  | loss : 0.22828632640186655\n",
            "Epoch: 81  | loss : 0.22769878925598186\n",
            "Epoch: 82  | loss : 0.22713593554522352\n",
            "Epoch: 83  | loss : 0.22659567317263354\n",
            "Epoch: 84  | loss : 0.22607602805108118\n",
            "Epoch: 85  | loss : 0.22557515946029424\n",
            "Epoch: 86  | loss : 0.22509137024469228\n",
            "Epoch: 87  | loss : 0.22462311215475125\n",
            "Epoch: 88  | loss : 0.22416898684719236\n",
            "Epoch: 89  | loss : 0.22372774320187913\n",
            "Epoch: 90  | loss : 0.22329827168274585\n",
            "Epoch: 91  | loss : 0.22287959647450875\n",
            "Epoch: 92  | loss : 0.22247086608066727\n",
            "Epoch: 93  | loss : 0.22207134298785516\n",
            "Epoch: 94  | loss : 0.22168039290262492\n",
            "Epoch: 95  | loss : 0.22129747396254035\n",
            "Epoch: 96  | loss : 0.2209221262238891\n",
            "Epoch: 97  | loss : 0.2205539616398933\n",
            "Epoch: 98  | loss : 0.22019265466929622\n",
            "Epoch: 99  | loss : 0.21983793359654547\n",
            "Epoch: 100  | loss : 0.21948957260051405\n",
            "Epoch: 101  | loss : 0.21914738457691516\n",
            "Epoch: 102  | loss : 0.21881121469785855\n",
            "Epoch: 103  | loss : 0.21848093467796376\n",
            "Epoch: 104  | loss : 0.21815643770797463\n",
            "Epoch: 105  | loss : 0.21783763401212589\n",
            "Epoch: 106  | loss : 0.21752444698334844\n",
            "Epoch: 107  | loss : 0.2172168098498009\n",
            "Epoch: 108  | loss : 0.21691466282662275\n",
            "Epoch: 109  | loss : 0.21661795070784992\n",
            "Epoch: 110  | loss : 0.21632662085492668\n",
            "Epoch: 111  | loss : 0.2160406215400668\n",
            "Epoch: 112  | loss : 0.21575990060482556\n",
            "Epoch: 113  | loss : 0.21548440439656696\n",
            "Epoch: 114  | loss : 0.2152140769480192\n",
            "Epoch: 115  | loss : 0.21494885936775443\n",
            "Epoch: 116  | loss : 0.21468868941212854\n",
            "Epoch: 117  | loss : 0.21443350121194085\n",
            "Epoch: 118  | loss : 0.21418322512974652\n",
            "Epoch: 119  | loss : 0.21393778772634026\n",
            "Epoch: 120  | loss : 0.21369711181737777\n",
            "Epoch: 121  | loss : 0.21346111660340478\n",
            "Epoch: 122  | loss : 0.21322971785867892\n",
            "Epoch: 123  | loss : 0.21300282816612395\n",
            "Epoch: 124  | loss : 0.21278035718751048\n",
            "Epoch: 125  | loss : 0.21256221195956493\n",
            "Epoch: 126  | loss : 0.2123482972081311\n",
            "Epoch: 127  | loss : 0.21213851567380787\n",
            "Epoch: 128  | loss : 0.21193276844363051\n",
            "Epoch: 129  | loss : 0.21173095528440253\n",
            "Epoch: 130  | loss : 0.21153297497420315\n",
            "Epoch: 131  | loss : 0.21133872562941844\n",
            "Epoch: 132  | loss : 0.21114810502538314\n",
            "Epoch: 133  | loss : 0.21096101090935637\n",
            "Epoch: 134  | loss : 0.21077734130513115\n",
            "Epoch: 135  | loss : 0.2105969948090719\n",
            "Epoch: 136  | loss : 0.21041987087778105\n",
            "Epoch: 137  | loss : 0.2102458701079532\n",
            "Epoch: 138  | loss : 0.210074894509244\n",
            "Epoch: 139  | loss : 0.20990684777117788\n",
            "Epoch: 140  | loss : 0.20974163552524624\n",
            "Epoch: 141  | loss : 0.20957916560339154\n",
            "Epoch: 142  | loss : 0.20941934829404263\n",
            "Epoch: 143  | loss : 0.20926209659674325\n",
            "Epoch: 144  | loss : 0.2091073264762279\n",
            "Epoch: 145  | loss : 0.20895495711649018\n",
            "Epoch: 146  | loss : 0.20880491117503217\n",
            "Epoch: 147  | loss : 0.20865711503700218\n",
            "Epoch: 148  | loss : 0.2085114990683892\n",
            "Epoch: 149  | loss : 0.20836799786680102\n",
            "Epoch: 150  | loss : 0.20822655050766453\n",
            "Epoch: 151  | loss : 0.20808710078292536\n",
            "Epoch: 152  | loss : 0.2079495974285263\n",
            "Epoch: 153  | loss : 0.20781399433614978\n",
            "Epoch: 154  | loss : 0.20768025074391322\n",
            "Epoch: 155  | loss : 0.20754833139998266\n",
            "Epoch: 156  | loss : 0.20741820669243344\n",
            "Epoch: 157  | loss : 0.2072898527382108\n",
            "Epoch: 158  | loss : 0.20716325142375058\n",
            "Epoch: 159  | loss : 0.20703839038979166\n",
            "Epoch: 160  | loss : 0.20691526295317902\n",
            "Epoch: 161  | loss : 0.20679386795907656\n",
            "Epoch: 162  | loss : 0.206674209557999\n",
            "Epoch: 163  | loss : 0.2065562969034847\n",
            "Epoch: 164  | loss : 0.20644014376802583\n",
            "Epoch: 165  | loss : 0.206325768077057\n",
            "Epoch: 166  | loss : 0.2062131913633095\n",
            "Epoch: 167  | loss : 0.20610243814658522\n",
            "Epoch: 168  | loss : 0.20599353524688116\n",
            "Epoch: 169  | loss : 0.20588651104166966\n",
            "Epoch: 170  | loss : 0.2057813946808383\n",
            "Epoch: 171  | loss : 0.20567821527517144\n",
            "Epoch: 172  | loss : 0.20557700107613508\n",
            "Epoch: 173  | loss : 0.20547777866595954\n",
            "Epoch: 174  | loss : 0.20538057217749878\n",
            "Epoch: 175  | loss : 0.20528540256299668\n",
            "Epoch: 176  | loss : 0.20519228692969516\n",
            "Epoch: 177  | loss : 0.2051012379582241\n",
            "Epoch: 178  | loss : 0.2050122634169984\n",
            "Epoch: 179  | loss : 0.20492536578258547\n",
            "Epoch: 180  | loss : 0.20484054197235654\n",
            "Epoch: 181  | loss : 0.20475778319192914\n",
            "Epoch: 182  | loss : 0.2046770748961439\n",
            "Epoch: 183  | loss : 0.2045983968587959\n",
            "Epoch: 184  | loss : 0.20452172334324273\n",
            "Epoch: 185  | loss : 0.2044470233634437\n",
            "Epoch: 186  | loss : 0.2043742610230604\n",
            "Epoch: 187  | loss : 0.20430339591896432\n",
            "Epoch: 188  | loss : 0.20423438359487522\n",
            "Epoch: 189  | loss : 0.20416717603083484\n",
            "Epoch: 190  | loss : 0.20410172215469932\n",
            "Epoch: 191  | loss : 0.2040379683627704\n",
            "Epoch: 192  | loss : 0.20397585903791757\n",
            "Epoch: 193  | loss : 0.20391533705500817\n",
            "Epoch: 194  | loss : 0.20385634426504654\n",
            "Epoch: 195  | loss : 0.2037988219510409\n",
            "Epoch: 196  | loss : 0.20374271125020768\n",
            "Epoch: 197  | loss : 0.2036879535386316\n",
            "Epoch: 198  | loss : 0.20363449077587087\n",
            "Epoch: 199  | loss : 0.20358226580823285\n",
            "Epoch: 200  | loss : 0.2035312226305054\n",
            "Epoch: 201  | loss : 0.20348130660682978\n",
            "Epoch: 202  | loss : 0.2034324646521233\n",
            "Epoch: 203  | loss : 0.20338464537603906\n",
            "Epoch: 204  | loss : 0.20333779919187026\n",
            "Epoch: 205  | loss : 0.20329187839310328\n",
            "Epoch: 206  | loss : 0.20324683720049594\n",
            "Epoch: 207  | loss : 0.20320263178264092\n",
            "Epoch: 208  | loss : 0.20315922025295918\n",
            "Epoch: 209  | loss : 0.20311656264599195\n",
            "Epoch: 210  | loss : 0.20307462087572375\n",
            "Epoch: 211  | loss : 0.20303335867849645\n",
            "Epoch: 212  | loss : 0.2029927415428563\n",
            "Epoch: 213  | loss : 0.20295273662845711\n",
            "Epoch: 214  | loss : 0.20291331267589582\n",
            "Epoch: 215  | loss : 0.2028744399091127\n",
            "Epoch: 216  | loss : 0.2028360899317562\n",
            "Epoch: 217  | loss : 0.2027982356186671\n",
            "Epoch: 218  | loss : 0.202760851003437\n",
            "Epoch: 219  | loss : 0.20272391116277863\n",
            "Epoch: 220  | loss : 0.2026873920982808\n",
            "Epoch: 221  | loss : 0.20265127061594598\n",
            "Epoch: 222  | loss : 0.20261552420379594\n",
            "Epoch: 223  | loss : 0.2025801309076966\n",
            "Epoch: 224  | loss : 0.20254506920548745\n",
            "Epoch: 225  | loss : 0.20251031787942775\n",
            "Epoch: 226  | loss : 0.20247585588694994\n",
            "Epoch: 227  | loss : 0.20244166222968088\n",
            "Epoch: 228  | loss : 0.20240771582073147\n",
            "Epoch: 229  | loss : 0.20237399535027034\n",
            "Epoch: 230  | loss : 0.20234047914947875\n",
            "Epoch: 231  | loss : 0.20230714505305314\n",
            "Epoch: 232  | loss : 0.20227397026055385\n",
            "Epoch: 233  | loss : 0.20224093119700115\n",
            "Epoch: 234  | loss : 0.20220800337330122\n",
            "Epoch: 235  | loss : 0.2021751612472349\n",
            "Epoch: 236  | loss : 0.20214237808594612\n",
            "Epoch: 237  | loss : 0.20210962583106476\n",
            "Epoch: 238  | loss : 0.20207687496781848\n",
            "Epoch: 239  | loss : 0.20204409439970747\n",
            "Epoch: 240  | loss : 0.20201125133054287\n",
            "Epoch: 241  | loss : 0.20197831115586315\n",
            "Epoch: 242  | loss : 0.20194523736594402\n",
            "Epoch: 243  | loss : 0.20191199146279265\n",
            "Epoch: 244  | loss : 0.20187853289365315\n",
            "Epoch: 245  | loss : 0.20184481900364312\n",
            "Epoch: 246  | loss : 0.2018108050101653\n",
            "Epoch: 247  | loss : 0.20177644400169853\n",
            "Epoch: 248  | loss : 0.20174168696344794\n",
            "Epoch: 249  | loss : 0.20170648283211176\n",
            "Epoch: 250  | loss : 0.2016707785817207\n",
            "Epoch: 251  | loss : 0.2016345193420892\n",
            "Epoch: 252  | loss : 0.20159764855092877\n",
            "Epoch: 253  | loss : 0.20156010814008735\n",
            "Epoch: 254  | loss : 0.20152183875573582\n",
            "Epoch: 255  | loss : 0.20148278001164033\n",
            "Epoch: 256  | loss : 0.20144287077394374\n",
            "Epoch: 257  | loss : 0.20140204947520288\n",
            "Epoch: 258  | loss : 0.20136025445478045\n",
            "Epoch: 259  | loss : 0.20131742432214159\n",
            "Epoch: 260  | loss : 0.20127349833915228\n",
            "Epoch: 261  | loss : 0.2012284168171885\n",
            "Epoch: 262  | loss : 0.20118212152470785\n",
            "Epoch: 263  | loss : 0.20113455610096728\n",
            "Epoch: 264  | loss : 0.20108566647174433\n",
            "Epoch: 265  | loss : 0.2010354012632432\n",
            "Epoch: 266  | loss : 0.2009837122107992\n",
            "Epoch: 267  | loss : 0.2009305545594855\n",
            "Epoch: 268  | loss : 0.20087588745423124\n",
            "Epoch: 269  | loss : 0.20081967431751072\n",
            "Epoch: 270  | loss : 0.2007618832129929\n",
            "Epoch: 271  | loss : 0.20070248719369427\n",
            "Epoch: 272  | loss : 0.20064146463309954\n",
            "Epoch: 273  | loss : 0.2005787995373421\n",
            "Epoch: 274  | loss : 0.20051448183588685\n",
            "Epoch: 275  | loss : 0.2004485076471691\n",
            "Epoch: 276  | loss : 0.20038087951441227\n",
            "Epoch: 277  | loss : 0.20031160660537928\n",
            "Epoch: 278  | loss : 0.20024070486824017\n",
            "Epoch: 279  | loss : 0.20016819713421\n",
            "Epoch: 280  | loss : 0.20009411315628892\n",
            "Epoch: 281  | loss : 0.20001848957256238\n",
            "Epoch: 282  | loss : 0.19994136978232635\n",
            "Epoch: 283  | loss : 0.19986280372400841\n",
            "Epoch: 284  | loss : 0.19978284754570372\n",
            "Epoch: 285  | loss : 0.19970156316221083\n",
            "Epoch: 286  | loss : 0.1996190176968097\n",
            "Epoch: 287  | loss : 0.19953528281152919\n",
            "Epoch: 288  | loss : 0.19945043393604447\n",
            "Epoch: 289  | loss : 0.19936454941220316\n",
            "Epoch: 290  | loss : 0.19927770957797106\n",
            "Epoch: 291  | loss : 0.19918999582069805\n",
            "Epoch: 292  | loss : 0.19910148963444155\n",
            "Epoch: 293  | loss : 0.19901227171912333\n",
            "Epoch: 294  | loss : 0.19892242116014888\n",
            "Epoch: 295  | loss : 0.19883201472564307\n",
            "Epoch: 296  | loss : 0.19874112631462068\n",
            "Epoch: 297  | loss : 0.19864982658347033\n",
            "Epoch: 298  | loss : 0.1985581827704068\n",
            "Epoch: 299  | loss : 0.19846625872845058\n",
            "Epoch: 300  | loss : 0.198374115167426\n",
            "Epoch: 301  | loss : 0.19828181009477663\n",
            "Epoch: 302  | loss : 0.19818939943390074\n",
            "Epoch: 303  | loss : 0.19809693778730594\n",
            "Epoch: 304  | loss : 0.1980044793002781\n",
            "Epoch: 305  | loss : 0.19791207856890713\n",
            "Epoch: 306  | loss : 0.19781979152447757\n",
            "Epoch: 307  | loss : 0.19772767621475978\n",
            "Epoch: 308  | loss : 0.19763579339255266\n",
            "Epoch: 309  | loss : 0.19754420681435975\n",
            "Epoch: 310  | loss : 0.19745298314947032\n",
            "Epoch: 311  | loss : 0.19736219140478956\n",
            "Epoch: 312  | loss : 0.19727190178673096\n",
            "Epoch: 313  | loss : 0.19718218395151885\n",
            "Epoch: 314  | loss : 0.19709310464152663\n",
            "Epoch: 315  | loss : 0.19700472476793754\n",
            "Epoch: 316  | loss : 0.1969170960757943\n",
            "Epoch: 317  | loss : 0.1968302576089289\n",
            "Epoch: 318  | loss : 0.19674423226730894\n",
            "Epoch: 319  | loss : 0.1966590238027132\n",
            "Epoch: 320  | loss : 0.19657461461436423\n",
            "Epoch: 321  | loss : 0.19649096467165317\n",
            "Epoch: 322  | loss : 0.1964080118015932\n",
            "Epoch: 323  | loss : 0.1963256734405322\n",
            "Epoch: 324  | loss : 0.19624384978137613\n",
            "Epoch: 325  | loss : 0.19616242807678244\n",
            "Epoch: 326  | loss : 0.19608128771658886\n",
            "Epoch: 327  | loss : 0.19600030561088674\n",
            "Epoch: 328  | loss : 0.19591936139415397\n",
            "Epoch: 329  | loss : 0.19583834202102546\n",
            "Epoch: 330  | loss : 0.19575714543607733\n",
            "Epoch: 331  | loss : 0.19567568314355485\n",
            "Epoch: 332  | loss : 0.19559388165006175\n",
            "Epoch: 333  | loss : 0.19551168287907722\n",
            "Epoch: 334  | loss : 0.1954290437446215\n",
            "Epoch: 335  | loss : 0.19534593511661946\n",
            "Epoch: 336  | loss : 0.19526234041592538\n",
            "Epoch: 337  | loss : 0.1951782540521198\n",
            "Epoch: 338  | loss : 0.19509367987411497\n",
            "Epoch: 339  | loss : 0.19500862975364538\n",
            "Epoch: 340  | loss : 0.1949231223736555\n",
            "Epoch: 341  | loss : 0.19483718225287142\n",
            "Epoch: 342  | loss : 0.1947508390068345\n",
            "Epoch: 343  | loss : 0.19466412682450632\n",
            "Epoch: 344  | loss : 0.19457708412692096\n",
            "Epoch: 345  | loss : 0.19448975336837993\n",
            "Epoch: 346  | loss : 0.19440218093941292\n",
            "Epoch: 347  | loss : 0.19431441713250291\n",
            "Epoch: 348  | loss : 0.19422651613509193\n",
            "Epoch: 349  | loss : 0.19413853601876804\n",
            "Epoch: 350  | loss : 0.19405053869825278\n",
            "Epoch: 351  | loss : 0.1939625898385364\n",
            "Epoch: 352  | loss : 0.19387475869316256\n",
            "Epoch: 353  | loss : 0.19378711786120073\n",
            "Epoch: 354  | loss : 0.19369974295490008\n",
            "Epoch: 355  | loss : 0.19361271217444623\n",
            "Epoch: 356  | loss : 0.19352610579060733\n",
            "Epoch: 357  | loss : 0.19344000554038493\n",
            "Epoch: 358  | loss : 0.19335449394495252\n",
            "Epoch: 359  | loss : 0.193269653563086\n",
            "Epoch: 360  | loss : 0.1931855661967783\n",
            "Epoch: 361  | loss : 0.1931023120686332\n",
            "Epoch: 362  | loss : 0.19301996899273127\n",
            "Epoch: 363  | loss : 0.19293861156183675\n",
            "Epoch: 364  | loss : 0.19285831037391662\n",
            "Epoch: 365  | loss : 0.19277913131992613\n",
            "Epoch: 366  | loss : 0.19270113495272262\n",
            "Epoch: 367  | loss : 0.1926243759538667\n",
            "Epoch: 368  | loss : 0.19254890271117797\n",
            "Epoch: 369  | loss : 0.19247475701545813\n",
            "Epoch: 370  | loss : 0.19240197388004873\n",
            "Epoch: 371  | loss : 0.19233058148220014\n",
            "Epoch: 372  | loss : 0.192260601220826\n",
            "Epoch: 373  | loss : 0.19219204788138988\n",
            "Epoch: 374  | loss : 0.1921249298955954\n",
            "Epoch: 375  | loss : 0.1920592496813329\n",
            "Epoch: 376  | loss : 0.19199500404703024\n",
            "Epoch: 377  | loss : 0.19193218464412556\n",
            "Epoch: 378  | loss : 0.19187077845174438\n",
            "Epoch: 379  | loss : 0.19181076827866356\n",
            "Epoch: 380  | loss : 0.19175213326917318\n",
            "Epoch: 381  | loss : 0.19169484940128273\n",
            "Epoch: 382  | loss : 0.19163888996774908\n",
            "Epoch: 383  | loss : 0.19158422603245437\n",
            "Epoch: 384  | loss : 0.19153082685665623\n",
            "Epoch: 385  | loss : 0.19147866029143532\n",
            "Epoch: 386  | loss : 0.19142769313428412\n",
            "Epoch: 387  | loss : 0.19137789144911885\n",
            "Epoch: 388  | loss : 0.1913292208500938\n",
            "Epoch: 389  | loss : 0.19128164675045425\n",
            "Epoch: 390  | loss : 0.1912351345782633\n",
            "Epoch: 391  | loss : 0.19118964996127305\n",
            "Epoch: 392  | loss : 0.1911451588834522\n",
            "Epoch: 393  | loss : 0.19110162781580048\n",
            "Epoch: 394  | loss : 0.19105902382408194\n",
            "Epoch: 395  | loss : 0.19101731465604788\n",
            "Epoch: 396  | loss : 0.1909764688105776\n",
            "Epoch: 397  | loss : 0.19093645559102337\n",
            "Epoch: 398  | loss : 0.19089724514483913\n",
            "Epoch: 399  | loss : 0.19085880849139922\n",
            "Epoch: 400  | loss : 0.1908211175397084\n",
            "Epoch: 401  | loss : 0.19078414509752412\n",
            "Epoch: 402  | loss : 0.19074786487323192\n",
            "Epoch: 403  | loss : 0.19071225147166002\n",
            "Epoch: 404  | loss : 0.19067728038485557\n",
            "Epoch: 405  | loss : 0.1906429279787319\n",
            "Epoch: 406  | loss : 0.1906091714763505\n",
            "Epoch: 407  | loss : 0.1905759889385198\n",
            "Epoch: 408  | loss : 0.19054335924228163\n",
            "Epoch: 409  | loss : 0.19051126205778174\n",
            "Epoch: 410  | loss : 0.19047967782394626\n",
            "Epoch: 411  | loss : 0.190448587723326\n",
            "Epoch: 412  | loss : 0.19041797365641142\n",
            "Epoch: 413  | loss : 0.19038781821567924\n",
            "Epoch: 414  | loss : 0.1903581046595865\n",
            "Epoch: 415  | loss : 0.1903288168866963\n",
            "Epoch: 416  | loss : 0.19029993941008586\n",
            "Epoch: 417  | loss : 0.1902714573321622\n",
            "Epoch: 418  | loss : 0.19024335631999162\n",
            "Epoch: 419  | loss : 0.19021562258122243\n",
            "Epoch: 420  | loss : 0.19018824284067318\n",
            "Epoch: 421  | loss : 0.19016120431763509\n",
            "Epoch: 422  | loss : 0.190134494703933\n",
            "Epoch: 423  | loss : 0.19010810214277324\n",
            "Epoch: 424  | loss : 0.1900820152084023\n",
            "Epoch: 425  | loss : 0.19005622288658996\n",
            "Epoch: 426  | loss : 0.19003071455594475\n",
            "Epoch: 427  | loss : 0.19000547997006823\n",
            "Epoch: 428  | loss : 0.18998050924054122\n",
            "Epoch: 429  | loss : 0.18995579282074673\n",
            "Epoch: 430  | loss : 0.18993132149051156\n",
            "Epoch: 431  | loss : 0.18990708634156764\n",
            "Epoch: 432  | loss : 0.18988307876381327\n",
            "Epoch: 433  | loss : 0.18985929043236707\n",
            "Epoch: 434  | loss : 0.18983571329539714\n",
            "Epoch: 435  | loss : 0.1898123395627124\n",
            "Epoch: 436  | loss : 0.18978916169510027\n",
            "Epoch: 437  | loss : 0.18976617239439245\n",
            "Epoch: 438  | loss : 0.1897433645942421\n",
            "Epoch: 439  | loss : 0.18972073145159835\n",
            "Epoch: 440  | loss : 0.18969826633885484\n",
            "Epoch: 441  | loss : 0.1896759628366551\n",
            "Epoch: 442  | loss : 0.1896538147273398\n",
            "Epoch: 443  | loss : 0.1896318159890078\n",
            "Epoch: 444  | loss : 0.18960996079017714\n",
            "Epoch: 445  | loss : 0.1895882434850225\n",
            "Epoch: 446  | loss : 0.18956665860916228\n",
            "Epoch: 447  | loss : 0.18954520087597893\n",
            "Epoch: 448  | loss : 0.1895238651734408\n",
            "Epoch: 449  | loss : 0.1895026465614038\n",
            "Epoch: 450  | loss : 0.18948154026936162\n",
            "Epoch: 451  | loss : 0.1894605416946174\n",
            "Epoch: 452  | loss : 0.18943964640084474\n",
            "Epoch: 453  | loss : 0.18941885011700454\n",
            "Epoch: 454  | loss : 0.1893981487365825\n",
            "Epoch: 455  | loss : 0.18937753831711163\n",
            "Epoch: 456  | loss : 0.18935701507993752\n",
            "Epoch: 457  | loss : 0.18933657541018659\n",
            "Epoch: 458  | loss : 0.189316215856894\n",
            "Epoch: 459  | loss : 0.18929593313324244\n",
            "Epoch: 460  | loss : 0.18927572411686683\n",
            "Epoch: 461  | loss : 0.18925558585017108\n",
            "Epoch: 462  | loss : 0.18923551554060739\n",
            "Epoch: 463  | loss : 0.1892155105608623\n",
            "Epoch: 464  | loss : 0.18919556844889404\n",
            "Epoch: 465  | loss : 0.18917568690776676\n",
            "Epoch: 466  | loss : 0.18915586380521876\n",
            "Epoch: 467  | loss : 0.18913609717291374\n",
            "Epoch: 468  | loss : 0.18911638520531343\n",
            "Epoch: 469  | loss : 0.18909672625811635\n",
            "Epoch: 470  | loss : 0.18907711884620645\n",
            "Epoch: 471  | loss : 0.18905756164106413\n",
            "Epoch: 472  | loss : 0.1890380534675851\n",
            "Epoch: 473  | loss : 0.18901859330026222\n",
            "Epoch: 474  | loss : 0.1889991802586977\n",
            "Epoch: 475  | loss : 0.18897981360240176\n",
            "Epoch: 476  | loss : 0.1889604927248588\n",
            "Epoch: 477  | loss : 0.18894121714683726\n",
            "Epoch: 478  | loss : 0.18892198650893555\n",
            "Epoch: 479  | loss : 0.18890280056336395\n",
            "Epoch: 480  | loss : 0.18888365916496772\n",
            "Epoch: 481  | loss : 0.1888645622615195\n",
            "Epoch: 482  | loss : 0.18884550988330495\n",
            "Epoch: 483  | loss : 0.18882650213205343\n",
            "Epoch: 484  | loss : 0.18880753916926182\n",
            "Epoch: 485  | loss : 0.18878862120398462\n",
            "Epoch: 486  | loss : 0.18876974848016465\n",
            "Epoch: 487  | loss : 0.18875092126359663\n",
            "Epoch: 488  | loss : 0.1887321398286184\n",
            "Epoch: 489  | loss : 0.1887134044446415\n",
            "Epoch: 490  | loss : 0.18869471536263327\n",
            "Epoch: 491  | loss : 0.18867607280166862\n",
            "Epoch: 492  | loss : 0.1886574769356811\n",
            "Epoch: 493  | loss : 0.1886389278805328\n",
            "Epoch: 494  | loss : 0.18862042568153445\n",
            "Epoch: 495  | loss : 0.18860197030153428\n",
            "Epoch: 496  | loss : 0.18858356160969822\n",
            "Epoch: 497  | loss : 0.18856519937109323\n",
            "Epoch: 498  | loss : 0.18854688323717714\n",
            "Epoch: 499  | loss : 0.18852861273728738\n"
          ]
        }
      ],
      "source": [
        "# struktur ANN\n",
        "input_layer = len(X[0])\n",
        "hidden_layer = 5\n",
        "output_layer = 3\n",
        "epoch = 500 # 500\n",
        "lr = .7\n",
        "Nguyen_widrow = True\n",
        "\n",
        "# training\n",
        "w_hidden, b_hidden, w_output, b_output, loss_values, acc_values = model_fit(hidden_layer, input_layer, output_layer, x_train, y_train, epoch, Nguyen_widrow)\n",
        "\n",
        "# testing\n",
        "accuracy, o_predict, o_predict2 = predict(x_test, w_hidden, b_hidden, w_output, b_output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3G7PBtbqnHNr",
        "outputId": "614b676d-2a4e-4921-d41e-e1796764156a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0, 1, 0], [0, 1, 0], [1, 0, 0], [1, 0, 0], [0, 0, 1], [0, 1, 0], [1, 0, 0], [0, 1, 0], [0, 1, 0], [1, 0, 0], [0, 1, 0], [0, 0, 1], [0, 0, 1], [0, 1, 0], [0, 1, 0], [0, 0, 1], [0, 1, 0], [0, 0, 1], [0, 1, 0], [0, 0, 1], [0, 1, 0], [0, 0, 1], [1, 0, 0], [0, 1, 0], [0, 1, 0], [0, 0, 1], [0, 0, 1], [0, 1, 0], [0, 0, 1], [0, 0, 1], [0, 0, 1], [1, 0, 0], [1, 0, 0], [0, 1, 0], [0, 1, 0], [0, 0, 1], [1, 0, 0], [0, 0, 1], [0, 0, 1], [0, 1, 0], [0, 1, 0], [0, 0, 1], [0, 1, 0], [0, 0, 1], [1, 0, 0], [0, 0, 1], [0, 1, 0], [1, 0, 0], [0, 0, 1], [0, 1, 0], [0, 1, 0], [0, 0, 1], [0, 1, 0], [0, 0, 1], [1, 0, 0], [0, 1, 0], [0, 0, 1], [1, 0, 0], [0, 1, 0], [0, 1, 0], [1, 0, 0], [0, 1, 0], [0, 0, 1], [0, 1, 0], [0, 1, 0], [0, 1, 0], [1, 0, 0], [0, 1, 0], [0, 0, 1], [1, 0, 0], [0, 0, 1], [0, 0, 1], [0, 1, 0], [1, 0, 0], [0, 1, 0], [0, 0, 1], [0, 1, 0], [1, 0, 0], [0, 1, 0], [0, 1, 0], [0, 1, 0], [0, 1, 0], [0, 1, 0], [0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 1, 0], [1, 0, 0], [0, 1, 0], [0, 1, 0], [0, 1, 0], [0, 1, 0], [0, 0, 1]]\n",
            "[[0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0], [1.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [1.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0], [1.0, 0.0, 0.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [1.0, 0.0, 0.0], [1.0, 0.0, 0.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 0.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [1.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [1.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [1.0, 0.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [1.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0]]\n",
            "[[14, 0, 4], [0, 42, 2], [4, 0, 27]]\n",
            "89.24731182795699\n",
            "86.77636580862388\n",
            "86.53198653198653\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import multilabel_confusion_matrix\n",
        "\n",
        "print(y_test)\n",
        "print(o_predict)\n",
        "# cm = multilabel_confusion_matrix(y_test, o_predict)\n",
        "# cma = multilabel_confusion_matrix(y_test, o_predict)\n",
        "# print(cma)\n",
        "\n",
        "temp = 0\n",
        "cm = [[0,0,0],[0,0,0],[0,0,0]]\n",
        "for i,y in enumerate(y_test):\n",
        "  if o_predict2[i].tolist().index(max(o_predict2[i]))==0: pred=[1,0,0]\n",
        "  elif o_predict2[i].tolist().index(max(o_predict2[i]))==1: pred=[0,1,0]\n",
        "  elif o_predict2[i].tolist().index(max(o_predict2[i]))==2: pred=[0,0,1]\n",
        "\n",
        "  if y_test[i]==[1,0,0] and pred==[1,0,0]: cm[0][0]+=1\n",
        "  elif y_test[i]==[1,0,0] and pred==[0,1,0]: cm[0][1]+=1\n",
        "  elif y_test[i]==[1,0,0] and pred==[0,0,1]: cm[0][2]+=1\n",
        "  elif y_test[i]==[0,1,0] and pred==[1,0,0]: cm[1][0]+=1\n",
        "  elif y_test[i]==[0,1,0] and pred==[0,1,0]: cm[1][1]+=1\n",
        "  elif y_test[i]==[0,1,0] and pred==[0,0,1]: cm[1][2]+=1\n",
        "  elif y_test[i]==[0,0,1] and pred==[1,0,0]: cm[2][0]+=1\n",
        "  elif y_test[i]==[0,0,1] and pred==[0,1,0]: cm[2][1]+=1\n",
        "  elif y_test[i]==[0,0,1] and pred==[0,0,1]: cm[2][2]+=1\n",
        "\n",
        "print(cm)\n",
        "\n",
        "accuracy = (cm[0][0] + cm[1][1] + cm[2][2])/ len(y_test)\n",
        "precision = ((cm[0][0]/(cm[0][0] + cm[0][1] + cm[0][2])) + (cm[1][1]/(cm[1][1] + cm[1][0] + cm[1][2])) + (cm[2][2]/(cm[2][2] + cm[2][0] + cm[2][1])))/3\n",
        "recall = ((cm[0][0]/(cm[0][0] + cm[1][0] + cm[2][0])) + (cm[1][1]/(cm[1][1] + cm[0][1] + cm[2][1])) + (cm[2][2]/(cm[2][2] + cm[0][2] + cm[1][2])))/3\n",
        "\n",
        "print(accuracy*100)\n",
        "print(precision*100)\n",
        "print(recall*100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 513
        },
        "id": "DUq3oQOwRbpz",
        "outputId": "ffd2cadf-f564-42a7-93d9-d33b311aa6ac"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plot1 = plt.figure(1)\n",
        "plt.plot(loss_values)\n",
        "plt.show()\n",
        "\n",
        "plot1 = plt.figure(1)\n",
        "plt.plot(acc_values)\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}